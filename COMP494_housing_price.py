# -*- coding: utf-8 -*-
"""COMP494FinalProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ndj8t2f8UTADAUw0HpWWgWGfn75GVvAA

# Data Importing and Pre-processing
"""

# Commented out IPython magic to ensure Python compatibility.
#import libraries needed
import pandas as pd
pd.set_option('display.max_columns', None)
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
from scipy.stats import norm, skew, probplot
from scipy.special import boxcox1p
import warnings
warnings.filterwarnings('ignore')
# %matplotlib inline

#read in file
housing_df = pd.read_csv('house_sales.csv')

#check number of rows and columns
housing_df.shape

#count the number of categorical variables and countinuous variables
cat_count = 0
for dtype in housing_df.dtypes:
    if dtype == 'object':
        cat_count = cat_count + 1
cont_count = housing_df.shape[1] - cat_count - 1
print(f"Number of Categorical Variables: {cat_count}")
print(f"Number of Continuous Variables: {cont_count}")

#print first 5 rows 
housing_df.head()

#check the column names
housing_df.columns

"""## Handling missing data

"""

#missing data
total = housing_df.isnull().sum().sort_values(ascending=False)
percent = (housing_df.isnull().sum()/housing_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])
missing_data.head(20)

missing_data['Percent'].head(10).plot(kind='barh', figsize = (20,10)).invert_yaxis() #top 10 missing columns
plt.xlabel("Missing Proportion")
plt.ylabel("Variable Name")
plt.title("Top 10 Proportion of Missing Data In Columns")
plt.show()

housing_df["bedrooms"] = housing_df.groupby("zipcode")["bedrooms"].transform(lambda x: x.fillna(x.median()))
housing_df["sqft_living"] = housing_df.groupby("zipcode")["sqft_living"].transform(lambda x: x.fillna(x.median()))
housing_df["bathrooms"] = housing_df.groupby("zipcode")["bathrooms"].transform(lambda x: x.fillna(x.median()))
housing_df["sqft_lot"] = housing_df.groupby("zipcode")["sqft_lot"].transform(lambda x: x.fillna(x.median()))

housing_df.head()

#Check remaining missing values if any 
all_data_na = (housing_df.isnull().sum() / len(housing_df)) * 100
all_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)
missing_data = pd.DataFrame({'Missing Ratio' :all_data_na})
missing_data.head()

housing_df['bedrooms'].value_counts()

"""## Handling Outliers"""

fig, ax = plt.subplots()
ax.scatter(x = housing_df['price'], y = housing_df['bedrooms'])
plt.ylabel('bedroom', fontsize=13)
plt.xlabel('Price', fontsize=13)
plt.show()

# cut the outlier 
#Deleting outliers
housing_df_new = housing_df.copy()
housing_df_new = housing_df_new.drop(housing_df_new[(housing_df_new['bedrooms']>15)].index)

#Check the graphic again
fig, ax = plt.subplots()
ax.scatter(housing_df_new['price'], housing_df_new['bedrooms'])
plt.ylabel('Bedrooms', fontsize=13)
plt.xlabel('Price', fontsize=13)
plt.show()

"""## Normalize Target Variable"""

sns.distplot(housing_df['price'] , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(housing_df['price'])
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],loc='best')
plt.ylabel('Frequency')
plt.title('Price distribution')

#Get also the QQ-plot
fig = plt.figure()
res = probplot(housing_df['price'], plot=plt)
plt.show()

# Select the numeric columns to include in normalization
numeric_columns = housing_df.select_dtypes(include=[np.number]).columns
exclude_columns = ['zipcode', 'lat', 'long']

# Apply log1p() function to the selected numeric columns
df_normalized = housing_df.copy()
df_normalized['price'] = df_normalized['price'].apply(np.log1p)

#Check the new distribution 
sns.distplot(df_normalized['price']  , fit=norm);

# Get the fitted parameters used by the function
(mu, sigma) = norm.fit(df_normalized['price'] )
print( '\n mu = {:.2f} and sigma = {:.2f}\n'.format(mu, sigma))

#Now plot the distribution
plt.legend(['Normal dist. ($\mu=$ {:.2f} and $\sigma=$ {:.2f} )'.format(mu, sigma)],
            loc='best')
plt.ylabel('Frequency')
plt.title('price distribution')

#Get also the QQ-plot
fig = plt.figure()
res = probplot(df_normalized['price'] , plot=plt)
plt.show()

"""# Data Analysis and Visualization"""

from sklearn.preprocessing import LabelEncoder

"""### Target Variable Scatterplots"""

sns.set()
cols = ['price', 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'floors']
sns.pairplot(df_normalized[cols], size = 2.5)
plt.show();

"""### Correlation Matrix"""

#Correlation map to see how features are correlated with SalePrice
corrmat = df_normalized.corr()
f, ax = plt.subplots(figsize=(15, 12))
sns.heatmap(corrmat, vmax=.8, square=True);

numeric_feats = df_normalized.dtypes[df_normalized.dtypes != "object"].index

# Check the skew of all numerical features
skewed_feats = df_normalized[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)
print("\nSkew in numerical features: \n")
skewness = pd.DataFrame({'Skew' :skewed_feats})
skewness.head(10)

skewness['Skew'].head(10).plot(kind='barh', figsize = (20,10)).invert_yaxis() #top 10 missing columns
plt.xlabel("Skew")
plt.ylabel("Variable Name")
plt.title("Top 10 Skewed Variables")
plt.show()

skewness = skewness[abs(skewness) > 0.75]
print("There are {} skewed numerical features to Box Cox transform (normalize)".format(skewness.shape[0]))

skewed_features = skewness.index
lam = 0.15
for feat in skewed_features:
    #all_data[feat] += 1
    housing_df[feat] = boxcox1p(housing_df[feat], lam)

df_normalized.head()

"""# Data Analytics"""

from sklearn.linear_model import Lasso
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
import xgboost as xgb
import lightgbm as lgb

# Split to training and testing set by 80% and 20% based on date
df_normalized['date'] = pd.to_datetime(df_normalized['date'])
df_normalized.sort_values('date', inplace=True)
split_index = int(len(df_normalized) * 0.8)

train_df = df_normalized[df_normalized.columns.difference(['id', 'date', 'price'])]
# Split the DataFrame into train and test based on the calculated index
x_train = train_df[:split_index]
y_train = df_normalized['price'][:split_index]
x_test = train_df[split_index:]
y_test = df_normalized['price'][split_index:]

x_train.shape

y_train.shape

x_test.shape

y_test.shape

lr_w_int = LinearRegression()
lr_no_int = LinearRegression(fit_intercept=False)

neigh = KNeighborsRegressor(n_neighbors=10)

dt = DecisionTreeRegressor(max_depth = 10)

rf = RandomForestRegressor(n_estimators=100)

model_xgb = xgb.XGBRegressor(max_depth=5, n_estimators=1000, learning_rate=0.01)

model_lgb = lgb.LGBMRegressor(learning_rate=0.01, max_depth=5, n_estimators=1000)

"""### Training on different models

#### Linear Regression with Intercept
"""

lr_w_int.fit(x_train, y_train)
lr_y_pred = lr_w_int.predict(x_test)

# Calculate RMSE
lr_mse = mean_squared_error(y_test, lr_y_pred)
lr_rmse = np.sqrt(lr_mse)

# Calculate standard deviation
lr_std = np.std(y_test - lr_y_pred)

# Print the RMSE and std
print(f"Linear Regression with Intercept has achieved RMSE:{lr_rmse} and STD: {lr_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": lr_y_pred})
y_compare_lr.head()

"""#### Linear Regression without Intercept"""

lr_no_int.fit(x_train, y_train)
lr_no_int_y_pred = lr_no_int.predict(x_test)

# Calculate RMSE
lr_no_mse = mean_squared_error(y_test, lr_no_int_y_pred)
lr_no_rmse = np.sqrt(lr_no_mse)

# Calculate standard deviation
lr_no_std = np.std(y_test - lr_no_int_y_pred)

# Print the RMSE and std
print(f"Linear Regression without Intercept has achieved RMSE:{lr_no_rmse} and STD: {lr_no_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": lr_no_int_y_pred})
y_compare_lr.head()

"""#### Nearest Neighbor"""

neigh.fit(x_train, y_train)
neigh_y_pred = neigh.predict(x_test)

# Calculate RMSE
neigh_mse = mean_squared_error(y_test, neigh_y_pred)
neigh_rmse = np.sqrt(neigh_mse)

# Calculate standard deviation
neigh_std = np.std(y_test - neigh_y_pred)

# Print the RMSE and std
print(f"Nearest Neighbor has achieved RMSE:{neigh_rmse} and STD: {neigh_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": neigh_y_pred})
y_compare_lr.head()

"""#### Decision Tree"""

dt.fit(x_train, y_train)
dt_y_pred = dt.predict(x_test)

# Calculate RMSE
dt_mse = mean_squared_error(y_test, dt_y_pred)
dt_rmse = np.sqrt(dt_mse)

# Calculate standard deviation
dt_std = np.std(y_test - dt_y_pred)

# Print the RMSE and std
print(f"Decision Tree has achieved RMSE:{dt_rmse} and STD: {dt_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": dt_y_pred})
y_compare_lr.head()

"""#### Random Forest"""

rf.fit(x_train, y_train)
rf_y_pred = rf.predict(x_test)

# Calculate RMSE
rf_mse = mean_squared_error(y_test, rf_y_pred)
rf_rmse = np.sqrt(rf_mse)

# Calculate standard deviation
rf_std = np.std(y_test - rf_y_pred)

# Print the RMSE and std
print(f"Random Forest has achieved RMSE:{rf_rmse} and STD: {rf_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": rf_y_pred})
y_compare_lr.head()

"""#### Xgboost Score"""

model_xgb.fit(x_train, y_train)
xgb_y_pred = model_xgb.predict(x_test)

# Calculate RMSE
xgb_mse = mean_squared_error(y_test, xgb_y_pred)
xgb_rmse = np.sqrt(xgb_mse)

# Calculate standard deviation
xgb_std = np.std(y_test - xgb_y_pred)

# Print the RMSE and std
print(f"Xgboost Score has achieved RMSE:{xgb_rmse} and STD: {xgb_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": xgb_y_pred})
y_compare_lr.head()

"""#### LGBM Score

"""

model_lgb.fit(x_train, y_train)
lgb_y_pred = model_lgb.predict(x_test)

# Calculate RMSE
lgb_mse = mean_squared_error(y_test, lgb_y_pred)
lgb_rmse = np.sqrt(lgb_mse)

# Calculate standard deviation
lgb_std = np.std(y_test - lgb_y_pred)

# Print the RMSE and std
print(f"LGBM Score has achieved RMSE:{lgb_rmse} and STD: {lgb_std}")

y_compare_lr = pd.DataFrame({"Actual": y_test, "Predicted": lgb_y_pred})
y_compare_lr.head()

"""## Visualize the Result"""

#plot RMSE and STD for each Algorithm
data = {'Linear (Intercept)':[lr_rmse, lr_std], 'Linear (No Intercept)':[lr_no_rmse, lr_no_std], 'XGBoost':[xgb_rmse, xgb_std], 'Random Forest': [rf_rmse,rf_std]
        , 'LightGBM': [lgb_rmse, lgb_std], 'Decision Tree': [dt_rmse,dt_std],'Nearest Neighbor': [neigh_rmse,neigh_std]}
data_df = pd.DataFrame(data=data).T.reset_index().sort_values(by = [0],ascending = True)
data_df.columns = ['Algorithm','RMSE','STD']

# creating the bar plot
data_df.plot(kind='bar',x = 'Algorithm', y = ['RMSE', 'STD'], figsize = (20,10), rot=0)
plt.xlabel("Algorithm",fontsize=20)
plt.ylabel("Root Mean Squared Error / Standard Deviation",fontsize=20)
plt.title("Root Mean Squared Error by Algorithm",fontsize=20)
plt.show()

"""### Variable Importance Plot"""

# model = model_xgb.fit(train_df, .price) #fit model on entire dataset to get variable importance since we fit it on each fold
feature_important = model_xgb.get_booster().get_score(importance_type='weight')

keys = list(feature_important.keys())
values = list(feature_important.values())

data = pd.DataFrame(data=values, index=keys, columns=["score"]).sort_values(by = "score", ascending=False)
data[:20].plot(kind='barh', figsize = (20,10)).invert_yaxis(); ## plot top 20 features
plt.xlabel("Feature Importance",fontsize=20)
plt.ylabel("Feature Name",fontsize=20)
plt.title("Feature Importance Plot",fontsize=20)
plt.show()

from prettytable import PrettyTable

sorted_feature_important = sorted(feature_important.items(), key=lambda x: x[1], reverse=True)
# Create a table to display feature importance
table = PrettyTable(["Feature", "Importance"])
for feature, importance in sorted_feature_important:
    table.add_row([feature, importance])

# Print the table
print(table)